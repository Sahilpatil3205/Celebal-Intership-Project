{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49079d5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecomposition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LatentDirichletAllocation, PCA ,TruncatedSVD\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# ðŸ“¦ Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, PCA ,TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# ðŸ”½ Download NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# ðŸ“¥ Step 1: Load Dataset\n",
    "print(\"Loading dataset...\")\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "docs = newsgroups.data\n",
    "\n",
    "# ðŸ§¹ Step 2: Clean and Preprocess Text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W+', ' ', text.lower())\n",
    "    words = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in words if word not in stop_words])\n",
    "\n",
    "# Limit to first 2000 documents for performance\n",
    "print(\"Cleaning text...\")\n",
    "docs_cleaned = [clean_text(doc) for doc in docs[:2000]]\n",
    "\n",
    "# ðŸ”  Step 3: TF-IDF Vectorization for K-Means\n",
    "print(\"Vectorizing with TF-IDF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=5, stop_words='english')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(docs_cleaned)\n",
    "\n",
    "# ðŸ”€ Step 4: Apply K-Means Clustering\n",
    "print(\"Applying K-Means clustering...\")\n",
    "kmeans = KMeans(n_clusters=20, random_state=42)\n",
    "kmeans.fit(X_tfidf)\n",
    "\n",
    "# ðŸ“‰ Step 5: Visualize Clusters with PCA\n",
    "print(\"Reducing dimensions for visualization...\")\n",
    "svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "scatter_data = svd.fit_transform(X_tfidf)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(scatter_data[:, 0], scatter_data[:, 1], c=kmeans.labels_, cmap='tab20', s=10)\n",
    "plt.title('K-Means Clustering of 20 Newsgroups (2D SVD Projection)')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.colorbar(label='Cluster ID')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ðŸ§® Step 6: Count Vectorizer for LDA\n",
    "print(\"Vectorizing with CountVectorizer for LDA...\")\n",
    "count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X_count = count_vectorizer.fit_transform(docs_cleaned)\n",
    "\n",
    "# ðŸ“Š Step 7: Apply LDA for Topic Modeling\n",
    "print(\"Applying Latent Dirichlet Allocation (LDA)...\")\n",
    "lda = LatentDirichletAllocation(n_components=10, max_iter=10, learning_method='online', random_state=42)\n",
    "lda.fit(X_count)\n",
    "\n",
    "# ðŸ“Œ Step 8: Print Top Words in Topics\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(f\"\\nðŸ”¹ Topic #{idx + 1}:\")\n",
    "        print(\", \".join([words[i] for i in topic.argsort()[:-top_n - 1:-1]]))\n",
    "\n",
    "print(\"\\nðŸ§  Top Words in LDA Topics:\")\n",
    "print_topics(lda, count_vectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568b54fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18fae07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a240a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408c4e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b4101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ebf04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
